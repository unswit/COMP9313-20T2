{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2\n",
    "In this lab, we will use PySpark to implement p-stable LSH on Euclidean Space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "from pyspark import SparkConf, SparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Spark Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf().setMaster(\"local[*]\").setAppName(\"lab2\")\n",
    "sc = SparkContext(conf = conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "We use random.randint to generate raw data and query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n, d = 500, 32 # n is the number of data points, d is the number of dimension of data\n",
    "data = [(i, [random.randint(0, 5) for _ in range(d)]) for i in range(n)]\n",
    "query = [random.randint(0, 5) for _ in range(d)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hash Function Generation\n",
    "We randomly generate normal vectors `a` and values `b` with uniform distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "L, K = 5, 20\n",
    "a = [[[random.normalvariate(0, 1) for _ in range(d)] for _ in range(K)] for _ in range(L)]\n",
    "b = [[random.uniform(0, 1) for _ in range(K)] for _ in range(L)]\n",
    "w = 100 # w is user specified paramter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate data rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:195"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_rdd = sc.parallelize(data)\n",
    "# We need to use data_rdd frequently, it's better to use cache() to store rdd in memory.\n",
    "data_rdd.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate L Hash Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is used to generate hash value according to hash function.\n",
    "project = lambda x, y, z:str(math.floor((sum([(x[i] * y[i]) for i in range(d)]) + z)/w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_hash (turn into string type) as key and data_id as value\n",
    "hash_table = lambda x, l:(\" \".join([project(x[1], a[l][k], b[l][k]) for k in range(K)]), x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This funciton is used to calculate L2 distance, we ignore the sqrt part for efficiency\n",
    "cal_distance = lambda x:(sum((i - j) ** 2 for i, j in zip(data[x][1], query)), x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### transform query into query hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_hash = [\" \".join([project(query, a[l][k], b[l][k]) for k in range(K)]) for l in range(L)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb_of_candidates: 29\n",
      "Most nearest point id: 124\n"
     ]
    }
   ],
   "source": [
    "# Create an empty rdd to store candidate id\n",
    "union_rdd = sc.emptyRDD()\n",
    "\n",
    "# We compute hash table for all the data and get candidate set\n",
    "for l in range(L):\n",
    "    # Get data hash\n",
    "    hash_rdd = data_rdd.map(lambda x:hash_table(x, l))\n",
    "    # Combine by key and form (key, {id_1, id_2, ...}) to generate hash_table\n",
    "    hash_table_rdd = hash_rdd.combineByKey(lambda x:{x}, lambda x, y: x | {y}, lambda x, y:x | y)\n",
    "    # Generate candidate set if H_l(q) == H_l(o)\n",
    "    cand_rdd = hash_table_rdd.filter(lambda x:x[0] == query_hash[l]).values().flatMap(lambda x:x)\n",
    "    # Union candidates\n",
    "    union_rdd = union_rdd.union(cand_rdd).distinct()\n",
    "\n",
    "# Count the number of candidates\n",
    "count = union_rdd.count()\n",
    "\n",
    "# Handle no candidate situation\n",
    "if count == 0:\n",
    "    print(\"None of data_hash collide with query_hash.\")\n",
    "else:\n",
    "    print(\"nb_of_candidates:\", count)\n",
    "    # For each candidate, we calculate the real distance between query_hash and sort it\n",
    "    sort_rdd = union_rdd.map(cal_distance).sortByKey()\n",
    "    # We use first() to get most nearest one to the query\n",
    "    closest_point = sort_rdd.first()[1]\n",
    "    print(\"Most nearest point id:\", closest_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# close spark context\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, we use `For Loop` to retrieve L hash_table. However, in the above implementation, retrieval in hash_table is not O(1). Instead, a transformation is needed.\n",
    "\n",
    "Could you find a more efficient way to reduce the number of transformations and avoid using `For Loop`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hint**: try to generate L hash table in one transformation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "COMP9313",
   "language": "python",
   "name": "comp9313"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
